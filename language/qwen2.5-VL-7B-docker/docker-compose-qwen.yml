services:
  # Service 1: vLLM Server (with Healthcheck)
  vllm:
    image: vllm/vllm-openai:v0.11.0-x86_64
    command:
      - "--model"
      - "Qwen/Qwen2.5-VL-7B-Instruct"
      - "--host"
      - "0.0.0.0"
      - "--port"
      - "8000"
    ports:
      - "8000:8000"
    
    # --- ADD THIS BLOCK to define when the server is "ready" ---
    healthcheck:
      # The vLLM server exposes a /health endpoint
      test: ["CMD-SHELL", "curl -f http://localhost:8000/health"]
      # How often to check (in seconds)
      interval: 10s
      # How long to wait for a response
      timeout: 5s
      # How many times to retry before marking as unhealthy
      retries: 5
      # !! CRITICAL: Wait 5 minutes before starting checks.
      # This gives the container time to download and load the model.
      start_period: 300s
    # --- END OF HEALTHCHECK BLOCK ---

    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: [gpu]
    volumes:
      - ~/.cache/huggingface:/root/.cache/huggingface

  # Service 2: Ubuntu Script Runner (with wait condition)
  script-runner:
    build:
      context: .
      dockerfile: server-mode.Dockerfile
    user: "${UID}:${GID}"
    volumes:
      - ../../loadgen:/loadgen/
      - ./:/app
    working_dir: /app
    
    environment:
      - HOME=/app
      - PATH=/app/.local/bin:$PATH
      - CHECKPOINT_PATH=Qwen/Qwen2.5-VL-7B-Instruct
      - DATASET_PATH=/app/datasets/mmmu_data.json
      - GPU_COUNT=1
      - SCENARIO=${SCENARIO}
    
    # --- ADD THIS BLOCK to make this service wait for vllm ---
    depends_on:
      vllm:
        condition: service_healthy
    
    command:
      - /bin/bash
      - -c
      - |
        echo 'vLLM server is healthy, starting script...'
        pip install -r /app/requirements-server-mode.txt
        pip install /loadgen/
        cd datasets && python generate_json_dataset.py && cd ../ &&
        time python main.py --args.scenario "$${SCENARIO}" \
              --args.model_path "$${CHECKPOINT_PATH}" \
              --args.batch_size 16 \
              --args.dtype bfloat16 \
              --args.user_conf user.conf \
              --args.total_sample_count 900 \
              --args.dataset_path "$${DATASET_PATH}" \
              --args.output_log_dir output \
              --args.tensor_parallel_size "$${GPU_COUNT}" \
              --args.num_workers 256 \
              --args.vllm 

networks:
  default:
    driver: bridge